{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww12640\viewh13420\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
Informational Entropy, a concept proposed by Claude Shannon, describes the amount of structure present in a data corpus. Consider and compare the entropy of an English novel, and the entropy of a similarly long text, made up of Unicode characters chosen equally at random. The first might have quite a bit of predictable underlying structure, the second, assuming UTF-8 encoding, is completely unpredictable beyond the 2-to-the-32nd options for a symbol at any given position. In this case there is essentially no possibility of useful data compression\'97 a process by which we reduce the size of a file through alternate, more efficient representations of the given data.\
\
The term \'93symbol\'94 here is fairly vague. It could mean a character, pixel color, or an event. For a coin flip the only 2 symbols are \'93heads\'94 or \'93tails\'94\'97 convenient then to represent it as a 1 or 0. We also know that for a fair coin, which has equal probability of landing \'93heads\'94 or \'93tails\'94, a sequence of results is completely random, and therefore each symbol gives us only 1 bit of new information. For the English language this isn\'92t quite the case, because following the certain letters there are varying probabilities for the following letter.\
\
Bits\'96per\'96symbol is how we measure entropy. A fair coin flip will have an entropy of 1 bit\'96per
\fs20 \'96
\fs24 symbol, an uncompressed representation a sequence of coin flips is just a sequence of 1\'92s or 0\'92s, with bit for each flip, which is also 1 bit\'96per\'96symbol. No Compression is possible!\
\
The goal of any compression algorithm is to get as close as possible to this natural limit on efficiency of communication imposed by entropy. Some algorithms can get quite close to this limit. We must ask\'85 what happens to the data when it undergoes compression?\
\
First and foremost it becomes shorter (we hope). Second, we expect the underlying data of whatever file we\'92re compressing to begin to look \'93more random\'94 as we get closer and closer to entropy. It\'92s important to note that it may look random to us, but a proper decoding algorithm should be able to get the original data back with little to no information lost.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 Huffman encoding is a lossless compression scheme to minimize a file\'92s data footprint. By distilling the patterns in our data to smaller sub\'96symbols, our losses compression schemes attempt to remove all predictability from the data, reducing the signal into its component noise. By calculating the relative probabilities of all symbols that occur in a corpus, we can devise a binary tree encoding by which the most probable symbols can be encoded with the fewest number of bits. The basis of this algorithm is still used today, albeit with more complicated optimizations.\
\
JPEG is a file format characterized by its sinusoidal artifacting and lossy compression algorithm. Through transforming the image in to the sum of many different waveforms, we may identify those waves which do not contribute much at all the overall quality of the image. we forget, those frequencies not significant enough to Len coarser definition. In much the same way that the automaton writes its own name, the JPEG interprets its visual world as the summation of atomic frequencies on the light spectrum. Blinded by vast brilliance, it must ignore that which it can\'92t process efficiently.\
\
JPEG compression is idempotent; compressing a file multiple times results in no change. The lossy nature of the algorithm produces a new image\'97 similar to the original\'97 but altered in such a way that if we were to compress this second image again, the affect of the compression is lossless.\
\
Monte Carlo methods illuminate the shadow of a function through random sampling. For each random sample, a new data point is created. For small values of N this might look random, but for larger N\'92s it will begin to reveal the distribution of our function. We\'92re doing quite a similar thing here. By mapping the compressed image back to our original color space, then iterating on the process, we hope to do the same here. Given a color space, where in this 0 to 1 spectrum might the JPEG artifacting eventually dissolve to?\
\
This was just one example of the process.}